{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import json\n",
    "from time import sleep\n",
    "import gym\n",
    "import numpy as np\n",
    "from src.utils import (\n",
    "    get_env, get_avg_slowdown, get_sjf_action,\n",
    "    get_possible_actions, finisihed_job_cnt)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils import shuffle\n",
    "from src.models.pointer_network import pointer_networks\n",
    "\n",
    "with open(\"configs/env.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "env = get_env(\"configs/env.json\")\n",
    "\n",
    "sds = []\n",
    "cnt = []\n",
    "\n",
    "observation = env.reset(seq_no=0 % 50)\n",
    "\n",
    "discount = 1.00\n",
    "max_job_length = config['max_job_length']\n",
    "capa = config['n_resource_slot_capacities']\n",
    "n_resources = len(config['n_resource_slot_capacities'])\n",
    "input_size = max_job_length * n_resources + 1\n",
    "state_size = np.prod(observation['machine'].shape)\n",
    "embedding_size = hidden_size = 32\n",
    "num_sample_batch = 15\n",
    "\n",
    "\n",
    "ptr_net = pointer_networks(\n",
    "    state_size, input_size, embedding_size, hidden_size,\n",
    "    max_job_length=max_job_length)\n",
    "\n",
    "adam = optim.RMSprop(ptr_net.parameters(), lr=0.001)\n",
    "loss = 0\n",
    "cnt = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajs(model, env, dg=False):\n",
    "    cc = np.random.randint(0, 100)\n",
    "    state = env.reset(seq_no=cc)\n",
    "    ob_obs = []\n",
    "    ac_actions = []\n",
    "    _ys = []\n",
    "    baseline = np.zeros(shape=(num_sample_batch, config['ep_force_stop']), dtype=np.float32)\n",
    "    batch_size = 16\n",
    "    ent_mean = []\n",
    "    for sample_batch in range(num_sample_batch):\n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        list_y = []\n",
    "        state = env.reset(seq_no=cc)\n",
    "        for t in range(config['ep_force_stop']):\n",
    "            if dg:\n",
    "                print(t)\n",
    "            action, action_indices, ent = model.get_action(state)\n",
    "            ent_mean.append(ent)\n",
    "            state2, reward, done, info = env.step(action)\n",
    "            obs.append(state)\n",
    "            actions.append(action_indices)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "            state = state2\n",
    "        disc_vec = np.array(rewards, dtype=float)\n",
    "        for i in range(1, len(rewards)):\n",
    "            disc_vec[i:] *= discount\n",
    "        for i in range(len(rewards)):\n",
    "            y_i = np.sum(disc_vec[i:]) / (discount ** i)\n",
    "            list_y.append(y_i)\n",
    "        _ys.append(list_y)\n",
    "        baseline[sample_batch, :len(list_y)] = list_y\n",
    "        ob_obs.append(obs)\n",
    "        ac_actions.append(actions)\n",
    "\n",
    "    baseline = np.mean(baseline, axis=0)\n",
    "    advs = []\n",
    "    for sb in range(num_sample_batch):\n",
    "        ys = np.array(_ys[sb])\n",
    "        ADV = ys - baseline[:len(ys)]\n",
    "        advs.append(ADV)\n",
    "    return [item for sublist in ob_obs for item in sublist], np.hstack(ac_actions), np.hstack(advs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exe = ThreadPoolExecutor(max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [get_env(\"configs/env.json\") for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  4.6548853\n",
      "[iter 0] avg rewards: -5.045\tavg slowdowns: 5.806, etnropies 0.69, fjc: 1337\n",
      "avg usages [0.40674505 0.4062005  0.42433168]\n",
      "loss :  2.3470194\n",
      "loss :  1.8873485\n",
      "loss :  3.7062366\n",
      "loss :  3.7135303\n",
      "loss :  2.1692045\n",
      "[iter 5] avg rewards: -4.484\tavg slowdowns: 4.812, etnropies 0.55, fjc: 1337\n",
      "avg usages [0.39820275 0.39766963 0.41542003]\n",
      "loss :  0.23871182\n",
      "loss :  2.813322\n",
      "loss :  2.1824841\n",
      "loss :  0.33193547\n",
      "loss :  0.70885754\n",
      "[iter 10] avg rewards: -4.090\tavg slowdowns: 4.099, etnropies 0.41, fjc: 1337\n",
      "avg usages [0.39756048 0.39702823 0.41475   ]\n",
      "loss :  1.6597219\n",
      "loss :  0.13069738\n",
      "loss :  0.5466099\n",
      "loss :  -0.4601334\n",
      "loss :  0.026335666\n",
      "[iter 15] avg rewards: -4.080\tavg slowdowns: 4.032, etnropies 0.40, fjc: 1337\n",
      "avg usages [0.40275735 0.40221814 0.42017157]\n",
      "loss :  -0.040803242\n",
      "loss :  -0.35644755\n",
      "loss :  -0.088743284\n",
      "loss :  0.043900866\n",
      "loss :  0.54006535\n",
      "[iter 20] avg rewards: -4.042\tavg slowdowns: 3.973, etnropies 0.34, fjc: 1337\n",
      "avg usages [0.39981752 0.39928224 0.41710462]\n",
      "loss :  -0.362885\n",
      "loss :  -0.12204954\n",
      "loss :  -0.12082711\n",
      "loss :  -2.4270654\n",
      "loss :  -1.2201135\n",
      "[iter 25] avg rewards: -4.091\tavg slowdowns: 4.116, etnropies 0.40, fjc: 1337\n",
      "avg usages [0.39692029 0.39638889 0.41408213]\n",
      "loss :  0.7033715\n",
      "loss :  -0.4538072\n",
      "loss :  -0.024383113\n",
      "loss :  0.37463456\n",
      "loss :  1.7173994\n",
      "[iter 30] avg rewards: -4.008\tavg slowdowns: 3.929, etnropies 0.29, fjc: 1337\n",
      "avg usages [0.3990085  0.3984743  0.41626062]\n",
      "loss :  -1.4557873\n",
      "loss :  -0.4543255\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1000):\n",
    "    adam.zero_grad()\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    fts = []\n",
    "    for jj in range(8):\n",
    "        fts.append(\n",
    "            exe.submit(get_trajs, ptr_net, envs[jj]))\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    adam.zero_grad()\n",
    "    for future in as_completed(fts):\n",
    "        obs, acs, advs = future.result()\n",
    "        for ob, ac, adv in zip(obs, acs, advs):\n",
    "            if len(ac) > 0:\n",
    "                t_loss = ptr_net.train_single(ob, ac, torch.as_tensor(adv, dtype=torch.float32))\n",
    "                loss += t_loss\n",
    "                cnt += 1\n",
    "    loss /= cnt\n",
    "    loss.backward()\n",
    "    adam.step()\n",
    "    print(\"loss : \", loss.detach().numpy())\n",
    "    if (iter >= 0) and (iter % 5 == 0):\n",
    "        usages = np.empty(shape=(0, 3))\n",
    "        ep_lengths = []\n",
    "        slowdowns = []\n",
    "        rews = []\n",
    "        ents = []\n",
    "        fjc = []\n",
    "        for i in range(30):\n",
    "            s = env.reset(seq_no=100 + i)\n",
    "            rr = []\n",
    "            action = []\n",
    "            for ep_len in range(config['ep_force_stop']):\n",
    "                action, action_indices, entropy = ptr_net.get_action(s, argmax=False)\n",
    "                usage = (100 - env.machine.avbl_slot[0]) / 100.0\n",
    "                usages = np.vstack([usages, usage])\n",
    "                s2, r, done, info = env.step(action)\n",
    "                ents.append(entropy)\n",
    "                rr.append(r)\n",
    "                if done:\n",
    "                    break\n",
    "                s = s2\n",
    "            fjc.append(finisihed_job_cnt(info))\n",
    "            rews.append(np.mean(rr))\n",
    "            slowdowns.append(get_avg_slowdown(info))\n",
    "\n",
    "        print(\"[iter %d] avg rewards: %0.3f\\tavg slowdowns: %0.3f, etnropies %0.2f, fjc: %d\" %\n",
    "                (iter, np.mean(rews), np.mean(slowdowns), np.mean(ents), np.sum(fjc)))\n",
    "        print(\"avg usages\", np.mean(usages, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
